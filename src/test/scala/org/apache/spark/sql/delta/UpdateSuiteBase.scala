/*
 * Copyright (2020) The Delta Lake Project Authors.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.delta

import java.io.File
import java.util.Locale

import scala.language.implicitConversions

import org.apache.spark.sql.delta.sources.DeltaSQLConf
import org.apache.hadoop.fs.Path
import org.scalatest.BeforeAndAfterEach

import org.apache.spark.sql.{AnalysisException, DataFrame, QueryTest, Row}
import org.apache.spark.sql.internal.SQLConf
import org.apache.spark.sql.test.{SharedSparkSession, SQLTestUtils}
import org.apache.spark.sql.types._
import org.apache.spark.util.Utils

abstract class UpdateSuiteBase
  extends QueryTest
  with SharedSparkSession
  with BeforeAndAfterEach
  with SQLTestUtils {
  import testImplicits._

  var tempDir: File = _

  var deltaLog: DeltaLog = _

  protected def tempPath = tempDir.getCanonicalPath

  protected def readDeltaTable(path: String): DataFrame = {
    spark.read.format("delta").load(path)
  }

  override def beforeEach() {
    super.beforeEach()
    tempDir = Utils.createTempDir()
    deltaLog = DeltaLog.forTable(spark, new Path(tempPath))
  }

  override def afterEach() {
    try {
      Utils.deleteRecursively(tempDir)
      DeltaLog.clearCache()
    } finally {
      super.afterEach()
    }
  }

  protected def executeUpdate(target: String, set: Seq[String], where: String): Unit = {
    executeUpdate(target, set.mkString(", "), where)
  }

  protected def executeUpdate(target: String, set: String, where: String = null): Unit

  protected def append(df: DataFrame, partitionBy: Seq[String] = Nil): Unit = {
    val writer = df.write.format("delta").mode("append")
    if (partitionBy.nonEmpty) {
      writer.partitionBy(partitionBy: _*)
    }
    writer.save(deltaLog.dataPath.toString)
  }

  implicit def jsonStringToSeq(json: String): Seq[String] = json.split("\n")

  val fileFormat: String = "parquet"

  protected def checkUpdate(
      condition: Option[String],
      setClauses: String,
      expectedResults: Seq[Row],
      tableName: Option[String] = None): Unit = {
    executeUpdate(tableName.getOrElse(s"delta.`$tempPath`"), setClauses, where = condition.orNull)
    checkAnswer(readDeltaTable(tempPath), expectedResults)
  }

  test("basic case") {
    append(Seq((2, 2), (1, 4), (1, 1), (0, 3)).toDF("key", "value"))
    checkUpdate(condition = None, setClauses = "key = 1, value = 2",
      expectedResults = Row(1, 2) :: Row(1, 2) :: Row(1, 2) :: Row(1, 2) :: Nil)
  }

  Seq(true, false).foreach { isPartitioned =>
    test(s"basic update - Delta table by path - Partition=$isPartitioned") {
      withTable("deltaTable") {
        val partitions = if (isPartitioned) "key" :: Nil else Nil
        append(Seq((2, 2), (1, 4), (1, 1), (0, 3)).toDF("key", "value"), partitions)

        checkUpdate(
          condition = Some("key >= 1"),
          setClauses = "value = key + value, key = key + 1",
          expectedResults = Row(0, 3) :: Row(2, 5) :: Row(2, 2) :: Row(3, 4) :: Nil,
          tableName = Some(s"delta.`$tempPath`"))
      }
    }
  }

  Seq(true, false).foreach { isPartitioned =>
    test(s"basic update - Delta table by name - Partition=$isPartitioned") {
      withTable("delta_table") {
        val partitionByClause = if (isPartitioned) "PARTITIONED BY (key)" else ""
        sql(s"""
             |CREATE TABLE delta_table(key INT, value INT)
             |USING delta
             |OPTIONS('path'='$tempPath')
             |$partitionByClause
           """.stripMargin)

        append(Seq((2, 2), (1, 4), (1, 1), (0, 3)).toDF("key", "value"))

        checkUpdate(
          condition = Some("key >= 1"),
          setClauses = "value = key + value, key = key + 1",
          expectedResults = Row(0, 3) :: Row(2, 5) :: Row(2, 2) :: Row(3, 4) :: Nil,
          tableName = Some("delta_table"))
      }
    }
  }

  Seq(true, false).foreach { skippingEnabled =>
    Seq(true, false).foreach { isPartitioned =>
      test(s"data and partition predicates - Partition=$isPartitioned Skipping=$skippingEnabled") {
        withSQLConf(DeltaSQLConf.DELTA_STATS_SKIPPING.key -> skippingEnabled.toString) {
          val partitions = if (isPartitioned) "key" :: Nil else Nil
          append(Seq((2, 2), (1, 4), (1, 1), (0, 3)).toDF("key", "value"), partitions)

          checkUpdate(condition = Some("key >= 1 and value != 4"),
            setClauses = "value = key + value, key = key + 5",
            expectedResults = Row(0, 3) :: Row(7, 4) :: Row(1, 4) :: Row(6, 2) :: Nil)
        }
      }
    }
  }

  Seq(true, false).foreach { isPartitioned =>
    test(s"SC-12276: table has null values - partitioned=$isPartitioned") {
      val partitions = if (isPartitioned) "key" :: Nil else Nil
      append(Seq(("a", 1), (null, 2), (null, 3), ("d", 4)).toDF("key", "value"), partitions)

      // predicate evaluates to null; no-op
      checkUpdate(condition = Some("key = null"),
        setClauses = "value = -1",
        expectedResults = Row("a", 1) :: Row(null, 2) :: Row(null, 3) :: Row("d", 4) :: Nil)

      checkUpdate(condition = Some("key = 'a'"),
        setClauses = "value = -1",
        expectedResults = Row("a", -1) :: Row(null, 2) :: Row(null, 3) :: Row("d", 4) :: Nil)

      checkUpdate(condition = Some("key is null"),
        setClauses = "value = -2",
        expectedResults = Row("a", -1) :: Row(null, -2) :: Row(null, -2) :: Row("d", 4) :: Nil)

      checkUpdate(condition = Some("key is not null"),
        setClauses = "value = -3",
        expectedResults = Row("a", -3) :: Row(null, -2) :: Row(null, -2) :: Row("d", -3) :: Nil)

      checkUpdate(condition = Some("key <=> null"),
        setClauses = "value = -4",
        expectedResults = Row("a", -3) :: Row(null, -4) :: Row(null, -4) :: Row("d", -3) :: Nil)
    }
  }

  test("basic case - condition is false") {
    append(Seq((2, 2), (1, 4), (1, 1), (0, 3)).toDF("key", "value"))
    checkUpdate(condition = Some("1 != 1"), setClauses = "key = 1, value = 2",
      expectedResults = Row(2, 2) :: Row(1, 4) :: Row(1, 1) :: Row(0, 3) :: Nil)
  }

  test("basic case - condition is true") {
    append(Seq((2, 2), (1, 4), (1, 1), (0, 3)).toDF("key", "value"))
    checkUpdate(condition = Some("1 = 1"), setClauses = "key = 1, value = 2",
      expectedResults = Row(1, 2) :: Row(1, 2) :: Row(1, 2) :: Row(1, 2) :: Nil)
  }

  Seq(true, false).foreach { isPartitioned =>
    test(s"basic update - without where - Partition=$isPartitioned") {
      val partitions = if (isPartitioned) "key" :: Nil else Nil
      append(Seq((2, 2), (1, 4), (1, 1), (0, 3)).toDF("key", "value"), partitions)

      checkUpdate(condition = None, setClauses = "key = 1, value = 2",
        expectedResults = Row(1, 2) :: Row(1, 2) :: Row(1, 2) :: Row(1, 2) :: Nil)
    }
  }

  Seq(true, false).foreach { isPartitioned =>
    test(s"basic update - without where and partial columns - Partition=$isPartitioned") {
      val partitions = if (isPartitioned) "key" :: Nil else Nil
      append(Seq((2, 2), (1, 4), (1, 1), (0, 3)).toDF("key", "value"), partitions)

      checkUpdate(condition = None, setClauses = "key = 1",
        expectedResults = Row(1, 1) :: Row(1, 2) :: Row(1, 3) :: Row(1, 4) :: Nil)
    }
  }

  Seq(true, false).foreach { isPartitioned =>
    test(s"basic update - without where and out-of-order columns - Partition=$isPartitioned") {
      val partitions = if (isPartitioned) "key" :: Nil else Nil
      append(Seq((2, 2), (1, 4), (1, 1), (0, 3)).toDF("key", "value"), partitions)

      checkUpdate(condition = None, setClauses = "value = 3, key = 1",
        expectedResults = Row(1, 3) :: Row(1, 3) :: Row(1, 3) :: Row(1, 3) :: Nil)
    }
  }

  Seq(true, false).foreach { isPartitioned =>
    test(s"basic update - without where and complex input - Partition=$isPartitioned") {
      val partitions = if (isPartitioned) "key" :: Nil else Nil
      append(Seq((2, 2), (1, 4), (1, 1), (0, 3)).toDF("key", "value"), partitions)

      checkUpdate(condition = None, setClauses = "value = key + 3, key = key + 1",
        expectedResults = Row(1, 3) :: Row(2, 4) :: Row(2, 4) :: Row(3, 5) :: Nil)
    }
  }

  Seq(true, false).foreach { isPartitioned =>
    test(s"basic update - with where - Partition=$isPartitioned") {
      val partitions = if (isPartitioned) "key" :: Nil else Nil
      append(Seq((2, 2), (1, 4), (1, 1), (0, 3)).toDF("key", "value"), partitions)

      checkUpdate(condition = Some("key = 1"), setClauses = "value = 3, key = 1",
        expectedResults = Row(1, 3) :: Row(2, 2) :: Row(0, 3) :: Row(1, 3) :: Nil)
    }
  }

  Seq(true, false).foreach { isPartitioned =>
    test(s"basic update - with where and complex input - Partition=$isPartitioned") {
      val partitions = if (isPartitioned) "key" :: Nil else Nil
      append(Seq((2, 2), (1, 4), (1, 1), (0, 3)).toDF("key", "value"), partitions)

      checkUpdate(condition = Some("key >= 1"), setClauses = "value = key + value, key = key + 1",
        expectedResults = Row(0, 3) :: Row(2, 5) :: Row(2, 2) :: Row(3, 4) :: Nil)
    }
  }

  Seq(true, false).foreach { isPartitioned =>
    test(s"basic update - with where and no row matched - Partition=$isPartitioned") {
      val partitions = if (isPartitioned) "key" :: Nil else Nil
      append(Seq((2, 2), (1, 4), (1, 1), (0, 3)).toDF("key", "value"), partitions)

      checkUpdate(condition = Some("key >= 10"), setClauses = "value = key + value, key = key + 1",
        expectedResults = Row(0, 3) :: Row(1, 1) :: Row(1, 4) :: Row(2, 2) :: Nil)
    }
  }

  Seq(true, false).foreach { isPartitioned =>
    test(s"type mismatch - Partition=$isPartitioned") {
      val partitions = if (isPartitioned) "key" :: Nil else Nil
      append(Seq((2, 2), (1, 4), (1, 1), (0, 3)).toDF("key", "value"), partitions)

      checkUpdate(condition = Some("key >= 1"),
        setClauses = "value = key + cast(value as String), key = key + '1'",
        expectedResults = Row(0, 3) :: Row(2, 5) :: Row(3, 4) :: Row(2, 2) :: Nil)
    }
  }

  Seq(true, false).foreach { isPartitioned =>
    test(s"set to null - Partition=$isPartitioned") {
      val partitions = if (isPartitioned) "key" :: Nil else Nil
      append(Seq((2, 2), (1, 4), (1, 1), (0, 3)).toDF("key", "value"), partitions)

      checkUpdate(condition = Some("key >= 1"),
        setClauses = "value = key, key = null + '1'",
        expectedResults = Row(0, 3) :: Row(null, 1) :: Row(null, 1) :: Row(null, 2) :: Nil)
    }
  }

  Seq(true, false).foreach { isPartitioned =>
    test(s"basic update - TypeCoercion twice - Partition=$isPartitioned") {
      val partitions = if (isPartitioned) "key" :: Nil else Nil
      append(Seq((99, 2), (100, 4), (101, 3)).toDF("key", "value"), partitions)

      checkUpdate(
        condition = Some("cast(key as long) * cast('1.0' as decimal(38, 18)) > 100"),
        setClauses = "value = -3",
        expectedResults = Row(100, 4) :: Row(101, -3) :: Row(99, 2) :: Nil)
    }
  }

  test("update cached table") {
    Seq((2, 2), (1, 4)).toDF("key", "value")
      .write.mode("overwrite").format("delta").save(tempPath)

    spark.read.format("delta").load(tempPath).cache()
    spark.read.format("delta").load(tempPath).collect()

    executeUpdate(s"delta.`$tempPath`", set = "key = 3")
    checkAnswer(spark.read.format("delta").load(tempPath), Row(3, 2) :: Row(3, 4) :: Nil)
  }

  test("different variations of column references") {
    append(Seq((99, 2), (100, 4), (101, 3), (102, 5)).toDF("key", "value"))

    spark.read.format("delta").load(tempPath).createOrReplaceTempView("tblName")

    checkUpdate(condition = Some("key = 99"), setClauses = "value = -1",
      Row(99, -1) :: Row(100, 4) :: Row(101, 3) :: Row(102, 5) :: Nil)
    checkUpdate(condition = Some("`key` = 100"), setClauses = "`value` = -1",
      Row(99, -1) :: Row(100, -1) :: Row(101, 3) :: Row(102, 5) :: Nil)
    checkUpdate(condition = Some("tblName.key = 101"), setClauses = "tblName.value = -1",
      Row(99, -1) :: Row(100, -1) :: Row(101, -1) :: Row(102, 5) :: Nil, Some("tblName"))
    checkUpdate(condition = Some("`tblName`.`key` = 102"), setClauses = "`tblName`.`value` = -1",
      Row(99, -1) :: Row(100, -1) :: Row(101, -1) :: Row(102, -1) :: Nil, Some("tblName"))
  }

  test("target columns can have db and table qualifiers") {
    withTable("target") {
      spark.read.json("""
          {"a": {"b.1": 1, "c.e": 'random'}, "d": 1}
          {"a": {"b.1": 3, "c.e": 'string'}, "d": 2}"""
        .split("\n").toSeq.toDS()).write.format("delta").saveAsTable("`target`")

      executeUpdate(
        target = "target",
        set = "`default`.`target`.a.`b.1` = -1, target.a.`c.e` = 'RANDOM'",
        where = "d = 1")

      checkAnswer(spark.table("target"),
        spark.read.json("""
            {"a": {"b.1": -1, "c.e": 'RANDOM'}, "d": 1}
            {"a": {"b.1": 3, "c.e": 'string'}, "d": 2}"""
          .split("\n").toSeq.toDS()))
    }
  }

  test("Negative case - non-delta target") {
    Seq((1, 1), (0, 3), (1, 5)).toDF("key1", "value")
      .write.mode("overwrite").format("parquet").save(tempPath)
    val e = intercept[AnalysisException] {
      executeUpdate(target = s"delta.`$tempPath`", set = "key1 = 3")
    }.getMessage
    assert(e.contains("UPDATE destination only supports Delta sources") ||
      e.contains("is not a Delta table") || e.contains("Incompatible format"))
  }

  test("Negative case - check target columns during analysis") {
    withTable("table") {
      sql("CREATE TABLE table (s int, t string) USING delta PARTITIONED BY (s)")
      var ae = intercept[AnalysisException] {
        executeUpdate("table", set = "column_doesnt_exist = 'San Francisco'", where = "t = 'a'")
      }
      assert(ae.message.contains("cannot resolve"))

      withSQLConf(SQLConf.CASE_SENSITIVE.key -> "false") {
        executeUpdate(target = "table", set = "S = 1, T = 'b'", where = "T = 'a'")
        ae = intercept[AnalysisException] {
          executeUpdate(target = "table", set = "S = 1, s = 'b'", where = "s = 1")
        }
        assert(ae.message.contains("There is a conflict from these SET columns"))
      }

      withSQLConf(SQLConf.CASE_SENSITIVE.key -> "true") {
        ae = intercept[AnalysisException] {
          executeUpdate(target = "table", set = "S = 1", where = "t = 'a'")
        }
        assert(ae.message.contains("cannot resolve"))

        ae = intercept[AnalysisException] {
          executeUpdate(target = "table", set = "S = 1, s = 'b'", where = "s = 1")
        }
        assert(ae.message.contains("cannot resolve"))

        // unresolved column in condition
        ae = intercept[AnalysisException] {
          executeUpdate(target = "table", set = "s = 1", where = "T = 'a'")
        }
        assert(ae.message.contains("cannot resolve"))
      }
    }
  }

  test("Negative case - UPDATE the child directory") {
    append(Seq((2, 2), (3, 2)).toDF("key", "value"), partitionBy = "key" :: Nil)
    val e = intercept[AnalysisException] {
      executeUpdate(
        target = s"delta.`$tempPath/key=2`",
        set = "key = 1, value = 2",
        where = "value = 2")
    }.getMessage
    assert(e.contains("Expect a full scan of Delta sources, but found a partial scan"))
  }

  test("Negative case - do not support subquery test") {
    append(Seq((2, 2), (1, 4), (1, 1), (0, 3)).toDF("key", "value"))
    Seq((2, 2), (1, 4), (1, 1), (0, 3)).toDF("c", "d").createOrReplaceTempView("source")

    // basic subquery
    val e0 = intercept[AnalysisException] {
      executeUpdate(target = s"delta.`$tempPath`",
        set = "key = 1",
        where = "key < (SELECT max(c) FROM source)")
    }.getMessage
    assert(e0.contains("Subqueries are not supported"))

    // subquery with EXISTS
    val e1 = intercept[AnalysisException] {
      executeUpdate(target = s"delta.`$tempPath`",
        set = "key = 1",
        where = "EXISTS (SELECT max(c) FROM source)")
    }.getMessage
    assert(e1.contains("Subqueries are not supported"))

    // subquery with NOT EXISTS
    val e2 = intercept[AnalysisException] {
      executeUpdate(target = s"delta.`$tempPath`",
        set = "key = 1",
        where = "NOT EXISTS (SELECT max(c) FROM source)")
    }.getMessage
    assert(e2.contains("Subqueries are not supported"))

    // subquery with IN
    val e3 = intercept[AnalysisException] {
      executeUpdate(target = s"delta.`$tempPath`",
        set = "key = 1",
        where = "key IN (SELECT max(c) FROM source)")
    }.getMessage
    assert(e3.contains("Subqueries are not supported"))

    // subquery with NOT IN
    val e4 = intercept[AnalysisException] {
      executeUpdate(target = s"delta.`$tempPath`",
        set = "key = 1",
        where = "key NOT IN (SELECT max(c) FROM source)")
    }.getMessage
    assert(e4.contains("Subqueries are not supported"))
  }

  test("nested data support") {
    // set a nested field
    checkUpdateJson(target = """
        {"a": {"c": {"d": 'random', "e": 'str'}, "g": 1}, "z": 10}
        {"a": {"c": {"d": 'random2', "e": 'str2'}, "g": 2}, "z": 20}""",
      updateWhere = "z = 10",
      set = "a.c.d = 'RANDOM'" :: Nil,
      expected = """
        {"a": {"c": {"d": 'RANDOM', "e": 'str'}, "g": 1}, "z": 10}
        {"a": {"c": {"d": 'random2', "e": 'str2'}, "g": 2}, "z": 20}""")

    // do nothing as condition has no match
    val unchanged = """
        {"a": {"c": {"d": 'RANDOM', "e": 'str'}, "g": 1}, "z": 10}
        {"a": {"c": {"d": 'random2', "e": 'str2'}, "g": 2}, "z": 20}"""
    checkUpdateJson(target = unchanged,
      updateWhere = "z = 30",
      set = "a.c.d = 'RANDOMMMMM'" :: Nil,
      expected = unchanged)

    // set multiple nested fields at different levels
    checkUpdateJson(
      target = """
          {"a": {"c": {"d": 'RANDOM', "e": 'str'}, "g": 1}, "z": 10}
          {"a": {"c": {"d": 'random2', "e": 'str2'}, "g": 2}, "z": 20}""",
      updateWhere = "z = 20",
      set = "a.c.d = 'RANDOM2'" :: "a.c.e = 'STR2'" :: "a.g = -2" :: "z = -20" :: Nil,
      expected = """
          {"a": {"c": {"d": 'RANDOM', "e": 'str'}, "g": 1}, "z": 10}
          {"a": {"c": {"d": 'RANDOM2', "e": 'STR2'}, "g": -2}, "z": -20}""")

    // set nested fields to null
    checkUpdateJson(
      target = """
          {"a": {"c": {"d": 'RANDOM', "e": 'str'}, "g": 1}, "z": 10}
          {"a": {"c": {"d": 'random2', "e": 'str2'}, "g": 2}, "z": 20}""",
      updateWhere = "a.c.d = 'random2'",
      set = "a.c = null" :: "a.g = null" :: Nil,
      expected = """
          {"a": {"c": {"d": 'RANDOM', "e": 'str'}, "g": 1}, "z": 10}
          {"a": {"c": null, "g": null}, "z": 20}""")

    // set a top struct type column to null
    checkUpdateJson(
      target = """
          {"a": {"c": {"d": 'RANDOM', "e": 'str'}, "g": 1}, "z": 10}
          {"a": {"c": {"d": 'random2', "e": 'str2'}, "g": 2}, "z": 20}""",
      updateWhere = "a.c.d = 'random2'",
      set = "a = null" :: Nil,
      expected = """
          {"a": {"c": {"d": 'RANDOM', "e": 'str'}, "g": 1}, "z": 10}
          {"a": null, "z": 20}""")

    // set a nested field using named_struct
    checkUpdateJson(
      target = """
          {"a": {"c": {"d": 'RANDOM', "e": 'str'}, "g": 1}, "z": 10}
          {"a": {"c": {"d": 'random2', "e": 'str2'}, "g": 2}, "z": 20}""",
      updateWhere = "a.g = 2",
      set = "a.c = named_struct('d', 'RANDOM2', 'e', 'STR2')" :: Nil,
      expected = """
          {"a": {"c": {"d": 'RANDOM', "e": 'str'}, "g": 1}, "z": 10}
          {"a": {"c": {"d": 'RANDOM2', "e": 'STR2'}, "g": 2}, "z": 20}""")

    // set an integer nested field with a string that can be casted into an integer
    checkUpdateJson(
      target = """
        {"a": {"c": {"d": 'random', "e": 'str'}, "g": 1}, "z": 10}
        {"a": {"c": {"d": 'random2', "e": 'str2'}, "g": 2}, "z": 20}""",
      updateWhere = "z = 10",
      set = "a.g = '-1'" :: "z = '30'" :: Nil,
      expected = """
        {"a": {"c": {"d": 'random', "e": 'str'}, "g": -1}, "z": 30}
        {"a": {"c": {"d": 'random2', "e": 'str2'}, "g": 2}, "z": 20}""")

    // set the nested data that has an Array field
    checkUpdateJson(
      target = """
          {"a": {"c": {"d": 'random', "e": [1, 11]}, "g": 1}, "z": 10}
          {"a": {"c": {"d": 'RANDOM2', "e": [2, 22]}, "g": 2}, "z": 20}""",
      updateWhere = "z = 20",
      set = "a.c.d = 'RANDOM22'" :: "a.g = -2" :: Nil,
      expected = """
          {"a": {"c": {"d": 'random', "e": [1, 11]}, "g": 1}, "z": 10}
          {"a": {"c": {"d": 'RANDOM22', "e": [2, 22]}, "g": -2}, "z": 20}""")

    // set an array field
    checkUpdateJson(
      target = """
          {"a": {"c": {"d": 'random', "e": [1, 11]}, "g": 1}, "z": 10}
          {"a": {"c": {"d": 'RANDOM22', "e": [2, 22]}, "g": -2}, "z": 20}""",
      updateWhere = "z = 10",
      set = "a.c.e = array(-1, -11)" ::  "a.g = -1" :: Nil,
      expected = """
          {"a": {"c": {"d": 'random', "e": [-1, -11]}, "g": -1}, "z": 10}
          {"a": {"c": {"d": 'RANDOM22', "e": [2, 22]}, "g": -2}, "z": 20}""")

    // set an array field as a top-level attribute
    checkUpdateJson(
      target = """
          {"a": [1, 11], "b": 'Z'}
          {"a": [2, 22], "b": 'Y'}""",
      updateWhere = "b = 'Z'",
      set = "a = array(-1, -11, -111)" :: Nil,
      expected = """
          {"a": [-1, -11, -111], "b": 'Z'}
          {"a": [2, 22], "b": 'Y'}""")
  }


  testQuietly("nested data - negative case") {
    val targetDF = spark.read.json("""
        {"a": {"c": {"d": 'random', "e": 'str'}, "g": 1}, "z": 10}
        {"a": {"c": {"d": 'random2', "e": 'str2'}, "g": 2}, "z": 20}"""
      .split("\n").toSeq.toDS())

    testAnalysisException(
      targetDF,
      set = "a.c = 'RANDOM2'" :: Nil,
      where = "z = 10",
      errMsgs = "data type mismatch" :: Nil)

    testAnalysisException(
      targetDF,
      set = "a.c.z = 'RANDOM2'" :: Nil,
      errMsgs = "No such struct field" :: Nil)

    testAnalysisException(
      targetDF,
      set = "a.c = named_struct('d', 'rand', 'e', 'str')" :: "a.c.d = 'RANDOM2'" :: Nil,
      errMsgs = "There is a conflict from these SET columns" :: Nil)

    testAnalysisException(
      targetDF,
      set =
        Seq("a = named_struct('c', named_struct('d', 'rand', 'e', 'str'))", "a.c.d = 'RANDOM2'"),
      errMsgs = "There is a conflict from these SET columns" :: Nil)

    val schema = new StructType().add("a", MapType(StringType, IntegerType))
    val mapData = spark.read.schema(schema).json(Seq("""{"a": {"b": 1}}""").toDS())
    testAnalysisException(
      mapData,
      set = "a.b = -1" :: Nil,
      errMsgs = "Updating nested fields is only supported for StructType" :: Nil)

    // Updating an ArrayStruct is not supported
    val arrayStructData = spark.read.json(Seq("""{"a": [{"b": 1}, {"b": 2}]}""").toDS())
    testAnalysisException(
      arrayStructData,
      set = "a.b = -1" :: Nil,
      errMsgs = "Updating nested fields is only supported for StructType" :: Nil)
  }


  protected def checkUpdateJson(
      target: Seq[String],
      source: Seq[String] = Nil,
      updateWhere: String,
      set: Seq[String],
      expected: Seq[String]): Unit = {
    withTempDir { dir =>
      withTempView("source") {
        def toDF(jsonStrs: Seq[String]) = spark.read.json(jsonStrs.toDS)
        toDF(target).write.format("delta").mode("overwrite").save(dir.toString)
        if (source.nonEmpty) {
          toDF(source).createOrReplaceTempView("source")
        }
        executeUpdate(s"delta.`$dir`", set, updateWhere)
        checkAnswer(readDeltaTable(dir.toString), toDF(expected))
      }
    }
  }

  protected def testAnalysisException(
      targetDF: DataFrame,
      set: Seq[String],
      where: String = null,
      errMsgs: Seq[String] = Nil) = {
    withTempDir { dir =>
      targetDF.write.format("delta").save(dir.toString)
      val e = intercept[AnalysisException] {
        executeUpdate(target = s"delta.`$dir`", set, where)
      }
      errMsgs.foreach { msg =>
        assert(e.getMessage.toLowerCase(Locale.ROOT).contains(msg.toLowerCase(Locale.ROOT)))
      }
    }
  }
}
